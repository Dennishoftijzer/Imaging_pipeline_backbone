
Code for root model, type=ScriptableAdapter:
class ScriptableAdapter(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  model : __torch__.detectron2.modeling.meta_arch.rcnn.___torch_mangle_409.GeneralizedRCNN
  training : Final[bool] = False
  def forward(self: __torch__.___torch_mangle_410.ScriptableAdapter,
    inputs: Tuple[Dict[str, Tensor]]) -> List[Dict[str, Tensor]]:
    model = self.model
    _0, = inputs
    instances = (model).inference([_0], None, False, )
    _1 = annotate(List[Dict[str, Tensor]], [])
    for _2 in range(torch.len(instances)):
      i = instances[_2]
      _3 = torch.append(_1, (i).get_fields())
    return _1

--------------------------------------------------------------------------------
Code for .model, type=GeneralizedRCNN:
class GeneralizedRCNN(Module):
  __parameters__ = []
  __buffers__ = ["pixel_mean", "pixel_std", ]
  pixel_mean : Tensor
  pixel_std : Tensor
  _is_full_backward_hook : Optional[bool]
  input_format : str
  vis_period : int
  backbone : __torch__.detectron2.modeling.backbone.fpn.___torch_mangle_389.FPN
  proposal_generator : __torch__.detectron2.modeling.proposal_generator.rpn.___torch_mangle_397.RPN
  roi_heads : __torch__.detectron2.modeling.roi_heads.roi_heads.___torch_mangle_408.StandardROIHeads
  training : Final[bool] = False
  def __device_getter(self: __torch__.detectron2.modeling.meta_arch.rcnn.___torch_mangle_409.GeneralizedRCNN) -> Device:
    pixel_mean = self.pixel_mean
    return ops.prim.device(pixel_mean)
  def forward(self: __torch__.detectron2.modeling.meta_arch.rcnn.___torch_mangle_409.GeneralizedRCNN,
    batched_inputs: List[Dict[str, Tensor]]) -> List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11]:
    _0 = (self).inference(batched_inputs, None, True, )
    return _0
  def inference(self: __torch__.detectron2.modeling.meta_arch.rcnn.___torch_mangle_409.GeneralizedRCNN,
    batched_inputs: List[Dict[str, Tensor]],
    detected_instances: Optional[List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11]]=None,
    do_postprocess: bool=True) -> List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11]:
    _1 = "AssertionError: Scripting is not supported for postprocess."
    _2 = uninitialized(List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11])
    images = (self).preprocess_image(batched_inputs, )
    backbone = self.backbone
    tensor = images.tensor
    features = (backbone).forward(tensor, )
    _3 = torch.__is__(detected_instances, None)
    if _3:
      proposal_generator = self.proposal_generator
      _4 = (proposal_generator).forward(images, features, None, )
      proposals, _5, = _4
      roi_heads = self.roi_heads
      _6 = (roi_heads).forward(images, features, proposals, None, )
      results0, _7, = _6
      results = results0
    else:
      detected_instances0 = unchecked_cast(List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11], detected_instances)
      detected_instances1 = annotate(List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11], [])
      for _8 in range(torch.len(detected_instances0)):
        x = detected_instances0[_8]
        _9 = (x).to((self).__device_getter(), )
        _10 = torch.append(detected_instances1, _9)
      roi_heads0 = self.roi_heads
      results1 = (roi_heads0).forward_with_given_boxes(features, detected_instances1, )
      results = results1
    if do_postprocess:
      ops.prim.RaiseException(_1)
      _11 = _2
    else:
      _11 = results
    return _11
  def preprocess_image(self: __torch__.detectron2.modeling.meta_arch.rcnn.___torch_mangle_409.GeneralizedRCNN,
    batched_inputs: List[Dict[str, Tensor]]) -> __torch__.detectron2.structures.image_list.ImageList:
    _12 = __torch__.detectron2.structures.image_list.___torch_mangle_444.from_tensors
    images = annotate(List[Tensor], [])
    for _13 in range(torch.len(batched_inputs)):
      x = batched_inputs[_13]
      _14 = torch.to(x["image"], (self).__device_getter())
      _15 = torch.append(images, _14)
    images0 = annotate(List[Tensor], [])
    for _16 in range(torch.len(images)):
      x0 = images[_16]
      pixel_mean = self.pixel_mean
      _17 = torch.sub(x0, pixel_mean)
      pixel_std = self.pixel_std
      _18 = torch.append(images0, torch.div(_17, pixel_std))
    backbone = self.backbone
    _19 = (backbone).__size_divisibility_getter()
    return _12(images0, _19, 0., )

--------------------------------------------------------------------------------
Code for .model.backbone, type=FPN:
class FPN(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  in_features : Tuple[str, str, str, str]
  _out_feature_strides : Dict[str, int]
  _out_features : List[str]
  _out_feature_channels : Dict[str, int]
  _size_divisibility : int
  top_block : __torch__.detectron2.modeling.backbone.fpn.___torch_mangle_345.LastLevelMaxPool
  bottom_up : __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_381.ResNet
  lateral_convs : __torch__.torch.nn.modules.container.___torch_mangle_386.ModuleList
  output_convs : __torch__.torch.nn.modules.container.___torch_mangle_388.ModuleList
  _fuse_type : Final[str] = "sum"
  training : Final[bool] = False
  def __size_divisibility_getter(self: __torch__.detectron2.modeling.backbone.fpn.___torch_mangle_389.FPN) -> int:
    return self._size_divisibility
  def forward(self: __torch__.detectron2.modeling.backbone.fpn.___torch_mangle_389.FPN,
    x: Tensor) -> Dict[str, Tensor]:
    _0 = __torch__.torch.nn.functional.___torch_mangle_43.interpolate
    bottom_up = self.bottom_up
    bottom_up_features = (bottom_up).forward(x, )
    results = annotate(List[Tensor], [])
    lateral_convs = self.lateral_convs
    _00 = getattr(lateral_convs, "0")
    in_features = self.in_features
    _1 = bottom_up_features[(in_features)[-1]]
    prev_features = (_00).forward(_1, )
    output_convs = self.output_convs
    _01 = getattr(output_convs, "0")
    _2 = torch.append(results, (_01).forward(prev_features, ))
    lateral_convs0 = self.lateral_convs
    _10 = getattr(lateral_convs0, "1")
    _20 = getattr(lateral_convs0, "2")
    _3 = getattr(lateral_convs0, "3")
    output_convs0 = self.output_convs
    _11 = getattr(output_convs0, "1")
    _21 = getattr(output_convs0, "2")
    _30 = getattr(output_convs0, "3")
    in_features0 = self.in_features
    features = (in_features0)[-2]
    features0 = bottom_up_features[features]
    top_down_features = _0(prev_features, None, 2., "nearest", None, None, False, )
    lateral_features = (_10).forward(features0, )
    prev_features0 = torch.add(lateral_features, top_down_features)
    torch.insert(results, 0, (_11).forward(prev_features0, ))
    in_features1 = self.in_features
    features1 = (in_features1)[-3]
    features2 = bottom_up_features[features1]
    top_down_features0 = _0(prev_features0, None, 2., "nearest", None, None, False, )
    lateral_features0 = (_20).forward(features2, )
    prev_features1 = torch.add(lateral_features0, top_down_features0)
    torch.insert(results, 0, (_21).forward(prev_features1, ))
    in_features2 = self.in_features
    features3 = (in_features2)[-4]
    features4 = bottom_up_features[features3]
    top_down_features1 = _0(prev_features1, None, 2., "nearest", None, None, False, )
    lateral_features1 = (_3).forward(features4, )
    prev_features2 = torch.add(lateral_features1, top_down_features1)
    torch.insert(results, 0, (_30).forward(prev_features2, ))
    top_block = self.top_block
    in_feature = top_block.in_feature
    _4 = torch.__contains__(bottom_up_features, in_feature)
    if _4:
      top_block0 = self.top_block
      in_feature0 = top_block0.in_feature
      top_block_in_feature = bottom_up_features[in_feature0]
    else:
      _out_features = self._out_features
      top_block1 = self.top_block
      in_feature1 = top_block1.in_feature
      _5 = torch.index(_out_features, in_feature1)
      top_block_in_feature = results[_5]
    top_block2 = self.top_block
    _6 = (top_block2).forward(top_block_in_feature, )
    torch.extend(results, _6)
    _out_features0 = self._out_features
    _7 = torch.eq(torch.len(_out_features0), torch.len(results))
    if _7:
      pass
    else:
      ops.prim.RaiseException("AssertionError: ")
    _8 = annotate(Dict[str, Tensor], {})
    _out_features1 = self._out_features
    _9 = [torch.len(_out_features1), torch.len(results)]
    for _12 in range(ops.prim.min(_9)):
      f = _out_features1[_12]
      res = results[_12]
      torch._set_item(_8, f, res)
    return _8

--------------------------------------------------------------------------------
Code for .model.backbone.top_block, type=LastLevelMaxPool:
class LastLevelMaxPool(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  num_levels : int
  in_feature : str
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.fpn.___torch_mangle_345.LastLevelMaxPool,
    x: Tensor) -> List[Tensor]:
    _0 = __torch__.torch.nn.functional.___torch_mangle_411._max_pool2d
    _1 = _0(x, [1, 1], [2, 2], [0, 0], [1, 1], False, False, )
    return [_1]

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up, type=ResNet:
class ResNet(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  num_classes : NoneType
  _out_feature_strides : Dict[str, int]
  _out_feature_channels : Dict[str, int]
  stage_names : Tuple[str, str, str, str]
  _out_features : List[str]
  stem : __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_348.BasicStem
  stages : __torch__.torch.nn.modules.container.___torch_mangle_380.ModuleList
  training : Final[bool] = False
  def __size_divisibility_getter(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_381.ResNet) -> int:
    return 0
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_381.ResNet,
    x: Tensor) -> Dict[str, Tensor]:
    _0 = "ResNet takes an input of shape (N, C, H, W). Got {} instead!"
    if torch.eq(torch.dim(x), 4):
      pass
    else:
      _1 = torch.add("AssertionError: ", torch.format(_0, torch.size(x)))
      ops.prim.RaiseException(_1)
    outputs = annotate(Dict[str, Tensor], {})
    stem = self.stem
    x0 = (stem).forward(x, )
    _out_features = self._out_features
    _2 = torch.__contains__(_out_features, "stem")
    if _2:
      torch._set_item(outputs, "stem", x0)
    else:
      pass
    stage_names = self.stage_names
    name, name0, name1, name2, = stage_names
    stages = self.stages
    _00 = getattr(stages, "0")
    _10 = getattr(stages, "1")
    _20 = getattr(stages, "2")
    _3 = getattr(stages, "3")
    x1 = (_00).forward(x0, )
    _out_features0 = self._out_features
    _4 = torch.__contains__(_out_features0, name)
    if _4:
      torch._set_item(outputs, name, x1)
    else:
      pass
    x2 = (_10).forward(x1, )
    _out_features1 = self._out_features
    _5 = torch.__contains__(_out_features1, name0)
    if _5:
      torch._set_item(outputs, name0, x2)
    else:
      pass
    x3 = (_20).forward(x2, )
    _out_features2 = self._out_features
    _6 = torch.__contains__(_out_features2, name1)
    if _6:
      torch._set_item(outputs, name1, x3)
    else:
      pass
    x4 = (_3).forward(x3, )
    _out_features3 = self._out_features
    _7 = torch.__contains__(_out_features3, name2)
    if _7:
      torch._set_item(outputs, name2, x4)
    else:
      pass
    return outputs

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stem, type=BasicStem:
class BasicStem(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_347.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_348.BasicStem,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_411._max_pool2d
    conv1 = self.conv1
    x0 = (conv1).forward(x, )
    x1 = torch.relu_(x0)
    x2 = _0(x1, [3, 3], [2, 2], [1, 1], [1, 1], False, False, )
    return x2

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stem.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 3
  kernel_size : Final[Tuple[int, int]] = (7, 7)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (2, 2)
  out_channels : Final[int] = 64
  padding : Final[Tuple[int, int]] = (3, 3)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_347.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [2, 2], [3, 3], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stem.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  __annotations__["0"] = __torch__.torch.nn.modules.container.___torch_mangle_355.Sequential
  __annotations__["1"] = __torch__.torch.nn.modules.container.___torch_mangle_363.Sequential
  __annotations__["2"] = __torch__.torch.nn.modules.container.___torch_mangle_371.Sequential
  __annotations__["3"] = __torch__.torch.nn.modules.container.___torch_mangle_379.Sequential
  training : Final[bool] = True
  def __len__(self: __torch__.torch.nn.modules.container.___torch_mangle_380.ModuleList) -> int:
    return 4

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  __annotations__["0"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_352.BottleneckBlock
  __annotations__["1"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_354.BottleneckBlock
  __annotations__["2"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_354.BottleneckBlock
  training : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_355.Sequential,
    input: Tensor) -> Tensor:
    _0 = getattr(self, "0")
    _1 = getattr(self, "1")
    _2 = getattr(self, "2")
    input0 = (_0).forward(input, )
    input1 = (_1).forward(input0, )
    return (_2).forward(input1, )
  def __len__(self: __torch__.torch.nn.modules.container.___torch_mangle_355.Sequential) -> int:
    return 3

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : __torch__.detectron2.layers.wrappers.___torch_mangle_349.Conv2d
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_350.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_351.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_349.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_352.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    shortcut = self.shortcut
    shortcut0 = (shortcut).forward(x, )
    out4 = torch.add_(out3, shortcut0)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.shortcut, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 64
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_349.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.shortcut.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 64
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 64
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_350.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 64
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 64
  padding : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_351.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 64
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_349.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.0.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.1, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_353.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_351.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_349.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_354.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.1.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 64
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_353.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.1.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.1.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 64
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 64
  padding : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_351.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.1.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.1.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 64
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_349.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.1.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.2, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_353.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_351.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_349.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_354.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.2.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 64
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_353.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.2.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.2.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 64
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 64
  padding : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_351.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.2.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.2.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 64
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_349.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.0.2.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  __annotations__["0"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_360.BottleneckBlock
  __annotations__["1"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_362.BottleneckBlock
  __annotations__["2"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_362.BottleneckBlock
  __annotations__["3"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_362.BottleneckBlock
  training : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_363.Sequential,
    input: Tensor) -> Tensor:
    _0 = getattr(self, "0")
    _1 = getattr(self, "1")
    _2 = getattr(self, "2")
    _3 = getattr(self, "3")
    input0 = (_0).forward(input, )
    input1 = (_1).forward(input0, )
    input2 = (_2).forward(input1, )
    return (_3).forward(input2, )
  def __len__(self: __torch__.torch.nn.modules.container.___torch_mangle_363.Sequential) -> int:
    return 4

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : __torch__.detectron2.layers.wrappers.___torch_mangle_356.Conv2d
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_357.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_358.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_359.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_360.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    shortcut = self.shortcut
    shortcut0 = (shortcut).forward(x, )
    out4 = torch.add_(out3, shortcut0)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.shortcut, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (2, 2)
  out_channels : Final[int] = 512
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_356.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [2, 2], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.shortcut.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (2, 2)
  out_channels : Final[int] = 128
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_357.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [2, 2], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 128
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 128
  padding : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_358.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 128
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 512
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_359.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.0.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.1, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_361.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_358.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_359.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_362.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.1.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 128
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_361.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.1.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.1.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 128
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 128
  padding : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_358.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.1.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.1.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 128
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 512
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_359.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.1.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.2, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_361.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_358.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_359.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_362.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.2.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 128
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_361.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.2.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.2.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 128
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 128
  padding : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_358.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.2.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.2.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 128
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 512
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_359.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.2.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.3, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_361.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_358.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_359.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_362.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.3.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 128
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_361.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.3.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.3.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 128
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 128
  padding : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_358.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.3.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.3.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 128
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 512
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_359.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.1.3.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  __annotations__["0"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_368.BottleneckBlock
  __annotations__["1"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_370.BottleneckBlock
  __annotations__["2"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_370.BottleneckBlock
  __annotations__["3"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_370.BottleneckBlock
  __annotations__["4"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_370.BottleneckBlock
  __annotations__["5"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_370.BottleneckBlock
  training : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_371.Sequential,
    input: Tensor) -> Tensor:
    _0 = getattr(self, "0")
    _1 = getattr(self, "1")
    _2 = getattr(self, "2")
    _3 = getattr(self, "3")
    _4 = getattr(self, "4")
    _5 = getattr(self, "5")
    input0 = (_0).forward(input, )
    input1 = (_1).forward(input0, )
    input2 = (_2).forward(input1, )
    input3 = (_3).forward(input2, )
    input4 = (_4).forward(input3, )
    return (_5).forward(input4, )
  def __len__(self: __torch__.torch.nn.modules.container.___torch_mangle_371.Sequential) -> int:
    return 6

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : __torch__.detectron2.layers.wrappers.___torch_mangle_364.Conv2d
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_365.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_366.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_367.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_368.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    shortcut = self.shortcut
    shortcut0 = (shortcut).forward(x, )
    out4 = torch.add_(out3, shortcut0)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.shortcut, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (2, 2)
  out_channels : Final[int] = 1024
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_364.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [2, 2], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.shortcut.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (2, 2)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_365.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [2, 2], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_366.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 1024
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_367.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.0.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.1, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_369.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_366.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_367.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_370.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.1.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_369.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.1.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.1.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_366.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.1.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.1.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 1024
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_367.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.1.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.2, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_369.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_366.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_367.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_370.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.2.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_369.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.2.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.2.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_366.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.2.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.2.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 1024
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_367.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.2.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.3, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_369.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_366.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_367.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_370.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.3.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_369.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.3.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.3.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_366.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.3.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.3.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 1024
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_367.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.3.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.4, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_369.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_366.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_367.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_370.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.4.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_369.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.4.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.4.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_366.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.4.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.4.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 1024
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_367.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.4.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.5, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_369.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_366.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_367.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_370.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.5.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_369.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.5.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.5.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_366.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.5.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.5.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 1024
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_367.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.2.5.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  __annotations__["0"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_376.BottleneckBlock
  __annotations__["1"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_378.BottleneckBlock
  __annotations__["2"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_378.BottleneckBlock
  training : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_379.Sequential,
    input: Tensor) -> Tensor:
    _0 = getattr(self, "0")
    _1 = getattr(self, "1")
    _2 = getattr(self, "2")
    input0 = (_0).forward(input, )
    input1 = (_1).forward(input0, )
    return (_2).forward(input1, )
  def __len__(self: __torch__.torch.nn.modules.container.___torch_mangle_379.Sequential) -> int:
    return 3

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : __torch__.detectron2.layers.wrappers.___torch_mangle_372.Conv2d
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_373.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_374.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_375.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_376.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    shortcut = self.shortcut
    shortcut0 = (shortcut).forward(x, )
    out4 = torch.add_(out3, shortcut0)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.shortcut, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (2, 2)
  out_channels : Final[int] = 2048
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_372.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [2, 2], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.shortcut.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (2, 2)
  out_channels : Final[int] = 512
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_373.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [2, 2], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 512
  padding : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_374.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 2048
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_375.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.0.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.1, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_377.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_374.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_375.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_378.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.1.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 2048
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 512
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_377.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.1.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.1.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 512
  padding : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_374.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.1.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.1.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 2048
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_375.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.1.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.2, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  stride : int
  shortcut : NoneType
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_377.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_374.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_375.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_378.BottleneckBlock,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    out = (conv1).forward(x, )
    out0 = torch.relu_(out)
    conv2 = self.conv2
    out1 = (conv2).forward(out0, )
    out2 = torch.relu_(out1)
    conv3 = self.conv3
    out3 = (conv3).forward(out2, )
    out4 = torch.add_(out3, x)
    return torch.relu_(out4)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.2.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 2048
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 512
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_377.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.2.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.2.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 512
  padding : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_374.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.2.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.2.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  activation : NoneType
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 2048
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_375.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    norm = self.norm
    return (norm).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stages.3.2.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  _is_full_backward_hook : Optional[bool]
  num_features : int
  eps : float
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_346.FrozenBatchNorm2d,
    x: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_412.batch_norm
    if ops.prim.requires_grad(x):
      weight = self.weight
      running_var = self.running_var
      eps = self.eps
      _2 = torch.rsqrt(torch.add(running_var, eps))
      scale = torch.mul(weight, _2)
      bias = self.bias
      running_mean = self.running_mean
      bias0 = torch.sub(bias, torch.mul(running_mean, scale))
      scale0 = torch.reshape(scale, [1, -1, 1, 1])
      bias1 = torch.reshape(bias0, [1, -1, 1, 1])
      out_dtype = ops.prim.dtype(x)
      _3 = torch.mul(x, torch.to(scale0, out_dtype))
      _4 = torch.add(_3, torch.to(bias1, out_dtype))
      _1 = _4
    else:
      running_mean0 = self.running_mean
      running_var0 = self.running_var
      weight0 = self.weight
      bias2 = self.bias
      eps0 = self.eps
      _5 = _0(x, running_mean0, running_var0, weight0, bias2, False, 0.10000000000000001, eps0, )
      _1 = _5
    return _1

--------------------------------------------------------------------------------
Code for .model.backbone.lateral_convs, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  __annotations__["0"] = __torch__.detectron2.layers.wrappers.___torch_mangle_382.Conv2d
  __annotations__["1"] = __torch__.detectron2.layers.wrappers.___torch_mangle_383.Conv2d
  __annotations__["2"] = __torch__.detectron2.layers.wrappers.___torch_mangle_384.Conv2d
  __annotations__["3"] = __torch__.detectron2.layers.wrappers.___torch_mangle_385.Conv2d
  training : Final[bool] = True
  def __len__(self: __torch__.torch.nn.modules.container.___torch_mangle_386.ModuleList) -> int:
    return 4

--------------------------------------------------------------------------------
Code for .model.backbone.lateral_convs.0, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  norm : NoneType
  activation : NoneType
  in_channels : Final[int] = 2048
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_382.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    return x0

--------------------------------------------------------------------------------
Code for .model.backbone.lateral_convs.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  norm : NoneType
  activation : NoneType
  in_channels : Final[int] = 1024
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_383.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    return x0

--------------------------------------------------------------------------------
Code for .model.backbone.lateral_convs.2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  norm : NoneType
  activation : NoneType
  in_channels : Final[int] = 512
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_384.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    return x0

--------------------------------------------------------------------------------
Code for .model.backbone.lateral_convs.3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  norm : NoneType
  activation : NoneType
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_385.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [0, 0], [1, 1])
    return x0

--------------------------------------------------------------------------------
Code for .model.backbone.output_convs, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  __annotations__["0"] = __torch__.detectron2.layers.wrappers.___torch_mangle_387.Conv2d
  __annotations__["1"] = __torch__.detectron2.layers.wrappers.___torch_mangle_387.Conv2d
  __annotations__["2"] = __torch__.detectron2.layers.wrappers.___torch_mangle_387.Conv2d
  __annotations__["3"] = __torch__.detectron2.layers.wrappers.___torch_mangle_387.Conv2d
  training : Final[bool] = True
  def __len__(self: __torch__.torch.nn.modules.container.___torch_mangle_388.ModuleList) -> int:
    return 4

--------------------------------------------------------------------------------
Code for .model.backbone.output_convs.0, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  norm : NoneType
  activation : NoneType
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_387.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    return x0

--------------------------------------------------------------------------------
Code for .model.backbone.output_convs.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  norm : NoneType
  activation : NoneType
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_387.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    return x0

--------------------------------------------------------------------------------
Code for .model.backbone.output_convs.2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  norm : NoneType
  activation : NoneType
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_387.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    return x0

--------------------------------------------------------------------------------
Code for .model.backbone.output_convs.3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  norm : NoneType
  activation : NoneType
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_387.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    return x0

--------------------------------------------------------------------------------
Code for .model.proposal_generator, type=RPN:
class RPN(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  in_features : List[str]
  anchor_matcher : __torch__.detectron2.modeling.matcher.Matcher
  box2box_transform : __torch__.detectron2.modeling.box_regression.Box2BoxTransform
  batch_size_per_image : int
  positive_fraction : float
  pre_nms_topk : Dict[bool, int]
  post_nms_topk : Dict[bool, int]
  nms_thresh : float
  min_box_size : float
  anchor_boundary_thresh : int
  loss_weight : Dict[str, float]
  box_reg_loss_type : str
  smooth_l1_beta : float
  rpn_head : __torch__.detectron2.modeling.proposal_generator.rpn.___torch_mangle_394.StandardRPNHead
  anchor_generator : __torch__.detectron2.modeling.anchor_generator.___torch_mangle_396.DefaultAnchorGenerator
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.proposal_generator.rpn.___torch_mangle_397.RPN,
    images: __torch__.detectron2.structures.image_list.ImageList,
    features: Dict[str, Tensor],
    gt_instances: Optional[List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11]]=None) -> Tuple[List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11], Dict[str, Tensor]]:
    features0 = annotate(List[Tensor], [])
    in_features = self.in_features
    for _0 in range(torch.len(in_features)):
      f = in_features[_0]
      _1 = torch.append(features0, features[f])
    anchor_generator = self.anchor_generator
    anchors = (anchor_generator).forward(features0, )
    rpn_head = self.rpn_head
    pred_objectness_logits, pred_anchor_deltas, = (rpn_head).forward(features0, )
    pred_objectness_logits0 = annotate(List[Tensor], [])
    for _2 in range(torch.len(pred_objectness_logits)):
      score = pred_objectness_logits[_2]
      _3 = torch.permute(score, [0, 2, 3, 1])
      _4 = torch.append(pred_objectness_logits0, torch.flatten(_3, 1))
    pred_anchor_deltas0 = annotate(List[Tensor], [])
    for _5 in range(torch.len(pred_anchor_deltas)):
      x = pred_anchor_deltas[_5]
      _6 = [(torch.size(x))[0], -1, 4, (torch.size(x))[-2], (torch.size(x))[-1]]
      _7 = torch.permute(torch.view(x, _6), [0, 3, 4, 1, 2])
      _8 = torch.append(pred_anchor_deltas0, torch.flatten(_7, 1, -2))
    losses = annotate(Dict[str, Tensor], {})
    image_sizes = images.image_sizes
    proposals = (self).predict_proposals(anchors, pred_objectness_logits0, pred_anchor_deltas0, image_sizes, )
    return (proposals, losses)
  def predict_proposals(self: __torch__.detectron2.modeling.proposal_generator.rpn.___torch_mangle_397.RPN,
    anchors: List[__torch__.detectron2.structures.boxes.Boxes],
    pred_objectness_logits: List[Tensor],
    pred_anchor_deltas: List[Tensor],
    image_sizes: List[Tuple[int, int]]) -> List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11]:
    _9 = __torch__.detectron2.modeling.proposal_generator.proposal_utils.___torch_mangle_418.find_top_rpn_proposals
    _10 = __torch__.torch.autograd.grad_mode.no_grad.__new__(__torch__.torch.autograd.grad_mode.no_grad)
    _11 = (_10).__init__()
    with _10:
      pred_proposals = (self)._decode_proposals(anchors, pred_anchor_deltas, )
      nms_thresh = self.nms_thresh
      pre_nms_topk = self.pre_nms_topk
      _12 = pre_nms_topk[False]
      post_nms_topk = self.post_nms_topk
      _13 = post_nms_topk[False]
      min_box_size = self.min_box_size
      _14 = _9(pred_proposals, pred_objectness_logits, image_sizes, nms_thresh, _12, _13, min_box_size, False, )
    return _14
  def _decode_proposals(self: __torch__.detectron2.modeling.proposal_generator.rpn.___torch_mangle_397.RPN,
    anchors: List[__torch__.detectron2.structures.boxes.Boxes],
    pred_anchor_deltas: List[Tensor]) -> List[Tensor]:
    N = (torch.size(pred_anchor_deltas[0]))[0]
    proposals = annotate(List[Tensor], [])
    _15 = [torch.len(anchors), torch.len(pred_anchor_deltas)]
    for _16 in range(ops.prim.min(_15)):
      anchors_i = anchors[_16]
      pred_anchor_deltas_i = pred_anchor_deltas[_16]
      tensor = anchors_i.tensor
      B = torch.size(tensor, 1)
      pred_anchor_deltas_i0 = torch.reshape(pred_anchor_deltas_i, [-1, B])
      tensor0 = anchors_i.tensor
      _17 = torch.expand(torch.unsqueeze(tensor0, 0), [N, -1, -1])
      anchors_i0 = torch.reshape(_17, [-1, B])
      box2box_transform = self.box2box_transform
      proposals_i = (box2box_transform).apply_deltas(pred_anchor_deltas_i0, anchors_i0, )
      _18 = torch.view(proposals_i, [N, -1, B])
      _19 = torch.append(proposals, _18)
    return proposals

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head, type=StandardRPNHead:
class StandardRPNHead(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.detectron2.layers.wrappers.___torch_mangle_391.Conv2d
  objectness_logits : __torch__.torch.nn.modules.conv.___torch_mangle_392.Conv2d
  anchor_deltas : __torch__.torch.nn.modules.conv.___torch_mangle_393.Conv2d
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.proposal_generator.rpn.___torch_mangle_394.StandardRPNHead,
    features: List[Tensor]) -> Tuple[List[Tensor], List[Tensor]]:
    pred_objectness_logits = annotate(List[Tensor], [])
    pred_anchor_deltas = annotate(List[Tensor], [])
    for _0 in range(torch.len(features)):
      x = features[_0]
      conv = self.conv
      t = (conv).forward(x, )
      objectness_logits = self.objectness_logits
      _1 = torch.append(pred_objectness_logits, (objectness_logits).forward(t, ))
      anchor_deltas = self.anchor_deltas
      _2 = torch.append(pred_anchor_deltas, (anchor_deltas).forward(t, ))
    _3 = (pred_objectness_logits, pred_anchor_deltas)
    return _3

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  norm : NoneType
  activation : __torch__.torch.nn.modules.activation.___torch_mangle_390.ReLU
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (3, 3)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 256
  padding : Final[Tuple[int, int]] = (1, 1)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_391.Conv2d,
    x: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    x0 = torch.conv2d(x, weight, bias, [1, 1], [1, 1], [1, 1])
    activation = self.activation
    return (activation).forward(x0, )

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head.conv.activation, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : NoneType
  inplace : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_390.ReLU,
    input: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_414.relu
    return _0(input, False, )

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head.objectness_logits, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 3
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.torch.nn.modules.conv.___torch_mangle_392.Conv2d,
    input: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    _0 = (self)._conv_forward(input, weight, bias, )
    return _0
  def _conv_forward(self: __torch__.torch.nn.modules.conv.___torch_mangle_392.Conv2d,
    input: Tensor,
    weight: Tensor,
    bias: Optional[Tensor]) -> Tensor:
    _1 = torch.conv2d(input, weight, bias, [1, 1], [0, 0], [1, 1])
    return _1

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head.anchor_deltas, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Optional[Tensor]
  training : bool
  _is_full_backward_hook : NoneType
  transposed : bool
  _reversed_padding_repeated_twice : List[int]
  in_channels : Final[int] = 256
  kernel_size : Final[Tuple[int, int]] = (1, 1)
  padding_mode : Final[str] = "zeros"
  stride : Final[Tuple[int, int]] = (1, 1)
  out_channels : Final[int] = 12
  padding : Final[Tuple[int, int]] = (0, 0)
  output_padding : Final[Tuple[int, int]] = (0, 0)
  dilation : Final[Tuple[int, int]] = (1, 1)
  groups : Final[int] = 1
  def forward(self: __torch__.torch.nn.modules.conv.___torch_mangle_393.Conv2d,
    input: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    _0 = (self)._conv_forward(input, weight, bias, )
    return _0
  def _conv_forward(self: __torch__.torch.nn.modules.conv.___torch_mangle_393.Conv2d,
    input: Tensor,
    weight: Tensor,
    bias: Optional[Tensor]) -> Tensor:
    _1 = torch.conv2d(input, weight, bias, [1, 1], [0, 0], [1, 1])
    return _1

--------------------------------------------------------------------------------
Code for .model.proposal_generator.anchor_generator, type=DefaultAnchorGenerator:
class DefaultAnchorGenerator(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  strides : List[int]
  num_features : int
  offset : float
  cell_anchors : __torch__.detectron2.modeling.anchor_generator.___torch_mangle_395.BufferList
  box_dim : Final[int] = 4
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.anchor_generator.___torch_mangle_396.DefaultAnchorGenerator,
    features: List[Tensor]) -> List[__torch__.detectron2.structures.boxes.Boxes]:
    grid_sizes = annotate(List[List[int]], [])
    for _0 in range(torch.len(features)):
      feature_map = features[_0]
      _1 = torch.slice(torch.size(feature_map), -2)
      _2 = torch.append(grid_sizes, _1)
    anchors_over_all_feature_maps = (self)._grid_anchors(grid_sizes, )
    _3 = annotate(List[__torch__.detectron2.structures.boxes.Boxes], [])
    _4 = torch.len(anchors_over_all_feature_maps)
    for _5 in range(_4):
      x = anchors_over_all_feature_maps[_5]
      _6 = __torch__.detectron2.structures.boxes.Boxes.__new__(__torch__.detectron2.structures.boxes.Boxes)
      _7 = (_6).__init__(x, )
      _8 = torch.append(_3, _6)
    return _3
  def _grid_anchors(self: __torch__.detectron2.modeling.anchor_generator.___torch_mangle_396.DefaultAnchorGenerator,
    grid_sizes: List[List[int]]) -> List[Tensor]:
    _9 = __torch__.detectron2.modeling.anchor_generator.___torch_mangle_417._create_grid_offsets
    anchors = annotate(List[Tensor], [])
    buffers = annotate(List[Tensor], [])
    cell_anchors = self.cell_anchors
    _0 = getattr(cell_anchors, "0")
    _1 = getattr(cell_anchors, "1")
    _2 = getattr(cell_anchors, "2")
    _3 = getattr(cell_anchors, "3")
    _4 = getattr(cell_anchors, "4")
    _10 = torch.append(buffers, _0)
    _11 = torch.append(buffers, _1)
    _12 = torch.append(buffers, _2)
    _13 = torch.append(buffers, _3)
    _14 = torch.append(buffers, _4)
    strides = self.strides
    _15 = [torch.len(grid_sizes), torch.len(strides), torch.len(buffers)]
    for _16 in range(ops.prim.min(_15)):
      size = grid_sizes[_16]
      stride = strides[_16]
      base_anchors = buffers[_16]
      offset = self.offset
      _17 = _9(size, stride, offset, ops.prim.device(base_anchors), )
      shift_x, shift_y, = _17
      _18 = [shift_x, shift_y, shift_x, shift_y]
      shifts = torch.stack(_18, 1)
      _19 = torch.view(shifts, [-1, 1, 4])
      _20 = torch.view(base_anchors, [1, -1, 4])
      _21 = torch.reshape(torch.add(_19, _20), [-1, 4])
      _22 = torch.append(anchors, _21)
    return anchors

--------------------------------------------------------------------------------
Code for .model.proposal_generator.anchor_generator.cell_anchors, type=BufferList:
class BufferList(Module):
  __parameters__ = []
  __buffers__ = ["0", "1", "2", "3", "4", ]
  __annotations__ = []
  __annotations__["0"] = Tensor
  __annotations__["1"] = Tensor
  __annotations__["2"] = Tensor
  __annotations__["3"] = Tensor
  __annotations__["4"] = Tensor
  _is_full_backward_hook : Optional[bool]
  training : Final[bool] = False

--------------------------------------------------------------------------------
Code for .model.roi_heads, type=StandardROIHeads:
class StandardROIHeads(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  batch_size_per_image : int
  positive_fraction : float
  num_classes : int
  proposal_matcher : __torch__.detectron2.modeling.matcher.Matcher
  proposal_append_gt : bool
  in_features : List[str]
  box_in_features : List[str]
  train_on_pred_boxes : bool
  box_pooler : __torch__.detectron2.modeling.poolers.___torch_mangle_400.ROIPooler
  box_head : __torch__.detectron2.modeling.roi_heads.box_head.___torch_mangle_404.FastRCNNConvFCHead
  box_predictor : __torch__.detectron2.modeling.roi_heads.fast_rcnn.___torch_mangle_407.FastRCNNOutputLayers
  keypoint_on : Final[bool] = False
  mask_on : Final[bool] = False
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.roi_heads.roi_heads.___torch_mangle_408.StandardROIHeads,
    images: __torch__.detectron2.structures.image_list.ImageList,
    features: Dict[str, Tensor],
    proposals: List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11],
    targets: Optional[List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11]]=None) -> Tuple[List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11], Dict[str, Tensor]]:
    pred_instances = (self)._forward_box(features, proposals, )
    pred_instances0 = (self).forward_with_given_boxes(features, pred_instances, )
    _0 = (pred_instances0, annotate(Dict[str, Tensor], {}))
    return _0
  def _forward_box(self: __torch__.detectron2.modeling.roi_heads.roi_heads.___torch_mangle_408.StandardROIHeads,
    features: Dict[str, Tensor],
    proposals: List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11]) -> List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11]:
    features0 = annotate(List[Tensor], [])
    box_in_features = self.box_in_features
    for _1 in range(torch.len(box_in_features)):
      f = box_in_features[_1]
      _2 = torch.append(features0, features[f])
    box_pooler = self.box_pooler
    _3 = annotate(List[__torch__.detectron2.structures.boxes.Boxes], [])
    for _4 in range(torch.len(proposals)):
      x = proposals[_4]
      _5 = torch.append(_3, (x).__proposal_boxes_getter())
    box_features = (box_pooler).forward(features0, _3, )
    box_head = self.box_head
    box_features0 = (box_head).forward(box_features, )
    box_predictor = self.box_predictor
    predictions = (box_predictor).forward(box_features0, )
    box_predictor0 = self.box_predictor
    _6 = (box_predictor0).inference(predictions, proposals, )
    pred_instances, _7, = _6
    return pred_instances
  def forward_with_given_boxes(self: __torch__.detectron2.modeling.roi_heads.roi_heads.___torch_mangle_408.StandardROIHeads,
    features: Dict[str, Tensor],
    instances: List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11]) -> List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11]:
    if (instances[0]).has("pred_boxes", ):
      _9 = (instances[0]).has("pred_classes", )
      _8 = _9
    else:
      _8 = False
    if _8:
      pass
    else:
      ops.prim.RaiseException("AssertionError: ")
    instances0 = (self)._forward_mask(features, instances, )
    instances1 = (self)._forward_keypoint(features, instances0, )
    return instances1
  def _forward_mask(self: __torch__.detectron2.modeling.roi_heads.roi_heads.___torch_mangle_408.StandardROIHeads,
    features: Dict[str, Tensor],
    instances: List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11]) -> List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11]:
    return instances
  def _forward_keypoint(self: __torch__.detectron2.modeling.roi_heads.roi_heads.___torch_mangle_408.StandardROIHeads,
    features: Dict[str, Tensor],
    instances: List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11]) -> List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11]:
    return instances

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler, type=ROIPooler:
class ROIPooler(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  output_size : Tuple[int, int]
  min_level : int
  max_level : int
  canonical_level : int
  canonical_box_size : int
  level_poolers : __torch__.torch.nn.modules.container.___torch_mangle_399.ModuleList
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.poolers.___torch_mangle_400.ROIPooler,
    x: List[Tensor],
    box_lists: List[__torch__.detectron2.structures.boxes.Boxes]) -> Tensor:
    _0 = "unequal value, num_level_assignments={}, but x is list of {} Tensors"
    _1 = "unequal value, x[0] batch dim 0 is {}, but box_list has length {}"
    _2 = __torch__.detectron2.modeling.poolers.___torch_mangle_436.convert_boxes_to_pooler_format
    _3 = __torch__.detectron2.modeling.poolers.___torch_mangle_438.assign_boxes_to_levels
    _4 = __torch__.detectron2.layers.wrappers.___torch_mangle_439.nonzero_tuple
    level_poolers = self.level_poolers
    num_level_assignments = (level_poolers).__len__()
    x0 = unchecked_cast(List[Tensor], x)
    x1 = unchecked_cast(List[Tensor], x0)
    box_lists0 = unchecked_cast(List[__torch__.detectron2.structures.boxes.Boxes], box_lists)
    _5 = torch.eq(torch.len(x1), num_level_assignments)
    if _5:
      pass
    else:
      _6 = torch.format(_0, num_level_assignments, torch.len(x1))
      ops.prim.RaiseException(torch.add("AssertionError: ", _6))
    _7 = torch.eq(torch.len(box_lists0), torch.size(x1[0], 0))
    if _7:
      pass
    else:
      _8 = torch.format(_1, torch.size(x1[0], 0), torch.len(box_lists0))
      ops.prim.RaiseException(torch.add("AssertionError: ", _8))
    if torch.eq(torch.len(box_lists0), 0):
      _10 = (torch.size(x1[0]))[1]
      output_size = self.output_size
      _11, _12, = output_size
      _13 = ops.prim.device(x1[0])
      _14 = ops.prim.dtype(x1[0])
      _15 = torch.zeros([0, _10, _11, _12], dtype=_14, layout=None, device=_13)
      _9 = _15
    else:
      pooler_fmt_boxes = _2(box_lists0, )
      _16 = torch.eq(num_level_assignments, 1)
      if _16:
        level_poolers0 = self.level_poolers
        _00 = getattr(level_poolers0, "0")
        _18 = (_00).forward(x1[0], pooler_fmt_boxes, )
        _17 = _18
      else:
        min_level = self.min_level
        max_level = self.max_level
        canonical_box_size = self.canonical_box_size
        canonical_level = self.canonical_level
        level_assignments = _3(box_lists0, min_level, max_level, canonical_box_size, canonical_level, )
        num_boxes = torch.size(pooler_fmt_boxes, 0)
        num_channels = (torch.size(x1[0]))[1]
        output_size0 = self.output_size
        output_size1 = (output_size0)[0]
        dtype = ops.prim.dtype(x1[0])
        device = ops.prim.device(x1[0])
        _19 = [num_boxes, num_channels, output_size1, output_size1]
        output = torch.zeros(_19, dtype=dtype, layout=None, device=device)
        level_poolers1 = self.level_poolers
        _01 = getattr(level_poolers1, "0")
        _110 = getattr(level_poolers1, "1")
        _20 = getattr(level_poolers1, "2")
        _30 = getattr(level_poolers1, "3")
        _21 = _4(torch.eq(level_assignments, 0), )
        inds = _21[0]
        _22 = annotate(List[Optional[Tensor]], [inds])
        pooler_fmt_boxes_level = torch.index(pooler_fmt_boxes, _22)
        _23 = (_01).forward(x1[0], pooler_fmt_boxes_level, )
        _24 = annotate(List[Optional[Tensor]], [inds])
        _25 = torch.index_put_(output, _24, _23)
        _26 = _4(torch.eq(level_assignments, 1), )
        inds0 = _26[0]
        _27 = annotate(List[Optional[Tensor]], [inds0])
        pooler_fmt_boxes_level0 = torch.index(pooler_fmt_boxes, _27)
        _28 = (_110).forward(x1[1], pooler_fmt_boxes_level0, )
        _29 = annotate(List[Optional[Tensor]], [inds0])
        _31 = torch.index_put_(output, _29, _28)
        _32 = _4(torch.eq(level_assignments, 2), )
        inds1 = _32[0]
        _33 = annotate(List[Optional[Tensor]], [inds1])
        pooler_fmt_boxes_level1 = torch.index(pooler_fmt_boxes, _33)
        _34 = (_20).forward(x1[2], pooler_fmt_boxes_level1, )
        _35 = annotate(List[Optional[Tensor]], [inds1])
        _36 = torch.index_put_(output, _35, _34)
        _37 = _4(torch.eq(level_assignments, 3), )
        inds2 = _37[0]
        _38 = annotate(List[Optional[Tensor]], [inds2])
        pooler_fmt_boxes_level2 = torch.index(pooler_fmt_boxes, _38)
        _39 = (_30).forward(x1[3], pooler_fmt_boxes_level2, )
        _40 = annotate(List[Optional[Tensor]], [inds2])
        _41 = torch.index_put_(output, _40, _39)
        _17 = output
      _9 = _17
    return _9

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  __annotations__["0"] = __torch__.detectron2.layers.roi_align.___torch_mangle_398.ROIAlign
  __annotations__["1"] = __torch__.detectron2.layers.roi_align.___torch_mangle_398.ROIAlign
  __annotations__["2"] = __torch__.detectron2.layers.roi_align.___torch_mangle_398.ROIAlign
  __annotations__["3"] = __torch__.detectron2.layers.roi_align.___torch_mangle_398.ROIAlign
  training : Final[bool] = False
  def __len__(self: __torch__.torch.nn.modules.container.___torch_mangle_399.ModuleList) -> int:
    return 4

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers.0, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  output_size : Tuple[int, int]
  spatial_scale : float
  sampling_ratio : int
  aligned : bool
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.roi_align.___torch_mangle_398.ROIAlign,
    input: Tensor,
    rois: Tensor) -> Tensor:
    _0 = __torch__.torchvision.ops.roi_align.___torch_mangle_432.roi_align
    if torch.eq(torch.dim(rois), 2):
      _1 = torch.eq(torch.size(rois, 1), 5)
    else:
      _1 = False
    if _1:
      pass
    else:
      ops.prim.RaiseException("AssertionError: ")
    if ops.prim.is_quantized(input):
      input0 = torch.dequantize(input)
    else:
      input0 = input
    _2 = torch.to(rois, ops.prim.dtype(input0))
    output_size = self.output_size
    spatial_scale = self.spatial_scale
    sampling_ratio = self.sampling_ratio
    aligned = self.aligned
    _3, _4, = output_size
    _5 = _0(input0, _2, [_3, _4], spatial_scale, sampling_ratio, aligned, )
    return _5

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers.1, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  output_size : Tuple[int, int]
  spatial_scale : float
  sampling_ratio : int
  aligned : bool
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.roi_align.___torch_mangle_398.ROIAlign,
    input: Tensor,
    rois: Tensor) -> Tensor:
    _0 = __torch__.torchvision.ops.roi_align.___torch_mangle_432.roi_align
    if torch.eq(torch.dim(rois), 2):
      _1 = torch.eq(torch.size(rois, 1), 5)
    else:
      _1 = False
    if _1:
      pass
    else:
      ops.prim.RaiseException("AssertionError: ")
    if ops.prim.is_quantized(input):
      input0 = torch.dequantize(input)
    else:
      input0 = input
    _2 = torch.to(rois, ops.prim.dtype(input0))
    output_size = self.output_size
    spatial_scale = self.spatial_scale
    sampling_ratio = self.sampling_ratio
    aligned = self.aligned
    _3, _4, = output_size
    _5 = _0(input0, _2, [_3, _4], spatial_scale, sampling_ratio, aligned, )
    return _5

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers.2, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  output_size : Tuple[int, int]
  spatial_scale : float
  sampling_ratio : int
  aligned : bool
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.roi_align.___torch_mangle_398.ROIAlign,
    input: Tensor,
    rois: Tensor) -> Tensor:
    _0 = __torch__.torchvision.ops.roi_align.___torch_mangle_432.roi_align
    if torch.eq(torch.dim(rois), 2):
      _1 = torch.eq(torch.size(rois, 1), 5)
    else:
      _1 = False
    if _1:
      pass
    else:
      ops.prim.RaiseException("AssertionError: ")
    if ops.prim.is_quantized(input):
      input0 = torch.dequantize(input)
    else:
      input0 = input
    _2 = torch.to(rois, ops.prim.dtype(input0))
    output_size = self.output_size
    spatial_scale = self.spatial_scale
    sampling_ratio = self.sampling_ratio
    aligned = self.aligned
    _3, _4, = output_size
    _5 = _0(input0, _2, [_3, _4], spatial_scale, sampling_ratio, aligned, )
    return _5

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers.3, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  output_size : Tuple[int, int]
  spatial_scale : float
  sampling_ratio : int
  aligned : bool
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.layers.roi_align.___torch_mangle_398.ROIAlign,
    input: Tensor,
    rois: Tensor) -> Tensor:
    _0 = __torch__.torchvision.ops.roi_align.___torch_mangle_432.roi_align
    if torch.eq(torch.dim(rois), 2):
      _1 = torch.eq(torch.size(rois, 1), 5)
    else:
      _1 = False
    if _1:
      pass
    else:
      ops.prim.RaiseException("AssertionError: ")
    if ops.prim.is_quantized(input):
      input0 = torch.dequantize(input)
    else:
      input0 = input
    _2 = torch.to(rois, ops.prim.dtype(input0))
    output_size = self.output_size
    spatial_scale = self.spatial_scale
    sampling_ratio = self.sampling_ratio
    aligned = self.aligned
    _3, _4, = output_size
    _5 = _0(input0, _2, [_3, _4], spatial_scale, sampling_ratio, aligned, )
    return _5

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head, type=FastRCNNConvFCHead:
class FastRCNNConvFCHead(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
  _output_size : int
  flatten : __torch__.torch.nn.modules.flatten.___torch_mangle_401.Flatten
  fc1 : __torch__.torch.nn.modules.linear.___torch_mangle_402.Linear
  fc_relu1 : __torch__.torch.nn.modules.activation.___torch_mangle_390.ReLU
  fc2 : __torch__.torch.nn.modules.linear.___torch_mangle_403.Linear
  fc_relu2 : __torch__.torch.nn.modules.activation.___torch_mangle_390.ReLU
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.roi_heads.box_head.___torch_mangle_404.FastRCNNConvFCHead,
    x: Tensor) -> Tensor:
    flatten = self.flatten
    fc1 = self.fc1
    fc_relu1 = self.fc_relu1
    fc2 = self.fc2
    fc_relu2 = self.fc_relu2
    x0 = (flatten).forward(x, )
    x1 = (fc1).forward(x0, )
    x2 = (fc_relu1).forward(x1, )
    x3 = (fc2).forward(x2, )
    return (fc_relu2).forward(x3, )
  def __len__(self: __torch__.detectron2.modeling.roi_heads.box_head.___torch_mangle_404.FastRCNNConvFCHead) -> int:
    return 5

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.flatten, type=Flatten:
class Flatten(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : NoneType
  end_dim : Final[int] = -1
  start_dim : Final[int] = 1
  def forward(self: __torch__.torch.nn.modules.flatten.___torch_mangle_401.Flatten,
    input: Tensor) -> Tensor:
    return torch.flatten(input, 1)

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.fc1, type=Linear:
class Linear(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : NoneType
  in_features : Final[int] = 12544
  out_features : Final[int] = 1024
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_402.Linear,
    input: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    return torch.linear(input, weight, bias)

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.fc_relu1, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : NoneType
  inplace : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_390.ReLU,
    input: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_414.relu
    return _0(input, False, )

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.fc2, type=Linear:
class Linear(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : NoneType
  in_features : Final[int] = 1024
  out_features : Final[int] = 1024
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_403.Linear,
    input: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    return torch.linear(input, weight, bias)

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.fc_relu2, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : NoneType
  inplace : Final[bool] = False
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_390.ReLU,
    input: Tensor) -> Tensor:
    _0 = __torch__.torch.nn.functional.___torch_mangle_414.relu
    return _0(input, False, )

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_predictor, type=FastRCNNOutputLayers:
class FastRCNNOutputLayers(Module):
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : Optional[bool]
  num_classes : int
  box2box_transform : __torch__.detectron2.modeling.box_regression.Box2BoxTransform
  smooth_l1_beta : float
  test_score_thresh : float
  test_nms_thresh : float
  test_topk_per_image : int
  box_reg_loss_type : str
  loss_weight : Dict[str, float]
  cls_score : __torch__.torch.nn.modules.linear.___torch_mangle_405.Linear
  bbox_pred : __torch__.torch.nn.modules.linear.___torch_mangle_406.Linear
  training : Final[bool] = False
  def forward(self: __torch__.detectron2.modeling.roi_heads.fast_rcnn.___torch_mangle_407.FastRCNNOutputLayers,
    x: Tensor) -> Tuple[Tensor, Tensor]:
    if torch.gt(torch.dim(x), 2):
      x0 = torch.flatten(x, 1)
    else:
      x0 = x
    cls_score = self.cls_score
    scores = (cls_score).forward(x0, )
    bbox_pred = self.bbox_pred
    proposal_deltas = (bbox_pred).forward(x0, )
    return (scores, proposal_deltas)
  def inference(self: __torch__.detectron2.modeling.roi_heads.fast_rcnn.___torch_mangle_407.FastRCNNOutputLayers,
    predictions: Tuple[Tensor, Tensor],
    proposals: List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11]) -> Tuple[List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11], List[Tensor]]:
    _0 = __torch__.detectron2.modeling.roi_heads.fast_rcnn.___torch_mangle_442.fast_rcnn_inference
    boxes = (self).predict_boxes(predictions, proposals, )
    scores = (self).predict_probs(predictions, proposals, )
    image_shapes = annotate(List[Tuple[int, int]], [])
    for _1 in range(torch.len(proposals)):
      x = proposals[_1]
      image_size = x.image_size
      _2 = torch.append(image_shapes, image_size)
    test_score_thresh = self.test_score_thresh
    test_nms_thresh = self.test_nms_thresh
    test_topk_per_image = self.test_topk_per_image
    _3 = _0(boxes, scores, image_shapes, test_score_thresh, test_nms_thresh, test_topk_per_image, )
    return _3
  def predict_boxes(self: __torch__.detectron2.modeling.roi_heads.fast_rcnn.___torch_mangle_407.FastRCNNOutputLayers,
    predictions: Tuple[Tensor, Tensor],
    proposals: List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11]) -> List[Tensor]:
    _4 = __torch__.detectron2.layers.wrappers.___torch_mangle_419.cat
    _5 = torch.__not__(bool(torch.len(proposals)))
    if _5:
      _6 = annotate(List[Tensor], [])
    else:
      _7, proposal_deltas, = predictions
      num_prop_per_image = annotate(List[int], [])
      for _8 in range(torch.len(proposals)):
        p = proposals[_8]
        _9 = torch.append(num_prop_per_image, (p).__len__())
      _10 = annotate(List[Tensor], [])
      for _11 in range(torch.len(proposals)):
        p0 = proposals[_11]
        tensor = (p0).__proposal_boxes_getter().tensor
        _12 = torch.append(_10, tensor)
      proposal_boxes = _4(_10, 0, )
      box2box_transform = self.box2box_transform
      predict_boxes = (box2box_transform).apply_deltas(proposal_deltas, proposal_boxes, )
      _13 = torch.split(predict_boxes, num_prop_per_image)
      _6 = _13
    return _6
  def predict_probs(self: __torch__.detectron2.modeling.roi_heads.fast_rcnn.___torch_mangle_407.FastRCNNOutputLayers,
    predictions: Tuple[Tensor, Tensor],
    proposals: List[__torch__.detectron2.export.torchscript_patch11.ScriptedInstances11]) -> List[Tensor]:
    _14 = __torch__.torch.nn.functional.___torch_mangle_440.softmax
    scores, _15, = predictions
    num_inst_per_image = annotate(List[int], [])
    for _16 in range(torch.len(proposals)):
      p = proposals[_16]
      _17 = torch.append(num_inst_per_image, (p).__len__())
    probs = _14(scores, -1, 3, None, )
    _18 = torch.split(probs, num_inst_per_image)
    return _18

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_predictor.cls_score, type=Linear:
class Linear(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : NoneType
  in_features : Final[int] = 1024
  out_features : Final[int] = 2
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_405.Linear,
    input: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    return torch.linear(input, weight, bias)

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_predictor.bbox_pred, type=Linear:
class Linear(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : NoneType
  in_features : Final[int] = 1024
  out_features : Final[int] = 4
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_406.Linear,
    input: Tensor) -> Tensor:
    weight = self.weight
    bias = self.bias
    return torch.linear(input, weight, bias)

--------------------------------------------------------------------------------